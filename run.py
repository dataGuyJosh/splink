from splink import splink_datasets, DuckDBAPI, block_on
from splink.exploratory import completeness_chart, profile_columns
from splink.blocking_analysis import (
    count_comparisons_from_blocking_rule,
    n_largest_blocks,
    cumulative_comparisons_to_be_scored_from_blocking_rules_chart
)
import altair as alt


# setup
db_api = DuckDBAPI()
alt.renderers.enable('browser')


# exploratory analysis
df = splink_datasets.fake_1000
df = df.drop(columns=["cluster"])
# print(df.head(5))

# completeness_chart(df, db_api=db_api).show()
# profile_columns(df, db_api=DuckDBAPI(), top_n=10, bottom_n=5).show()


# blocking
"""
specifying the types of matches to compare
e.g. only resolve when first name and DOB match between 2 entities
block_on('first_name', 'dob')

On large datasets, some blocking rules imply the creation of
trillions of record comparisons, which would cause a linkage job to fail.

Before using a blocking rule in a linkage job, it's therefore a good idea to
count the number of records it generates to ensure it is not too loose:
"""

def run_blocking_comp(block_on_expr):
    return count_comparisons_from_blocking_rule(
        table_or_tables=df,
        blocking_rule=block_on_expr,
        link_type="dedupe_only",
        db_api=db_api
    )

# print(
#     'First Initial & Last Name',
#     run_blocking_comp(block_on("substr(first_name, 1,1)", "surname")),
#     'First Name & DOB',
#     run_blocking_comp(block_on('first_name', 'dob')),
#     'First Name & Similar Last Name',
#     run_blocking_comp("""
#         l.first_name == r.first_name
#         AND LEVENSHTEIN(l.surname, r.surname) < 2
#     """),
#     sep='\n\n'
# )

"""
Finding 'worst offending' values for blocking rules
In this case, we can see that Olivers in London will result in 49 comparisons being generated.
This is acceptable on this small dataset, but on a larger dataset,
Olivers in London could be responsible for many million comparisons.
"""

# print(n_largest_blocks(
#     table_or_tables=df,
#     blocking_rule= block_on("city", "first_name"),
#     link_type="dedupe_only",
#     db_api=db_api,
#     n_largest=3
# ).as_pandas_dataframe())

"""
As noted above, it's usually a good idea to use multiple blocking rules.
It's therefore useful to know how many record comparisons will be generated when these rules are applied.

Since the same record comparison may be created by several blocking rules,
and Splink automatically deduplicates these comparisons,
we cannot simply total the number of comparisons generated by each rule individually.

Splink provides a chart that shows the marginal (additional) comparisons generated by each blocking rule, after deduplication:
"""

cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
    table_or_tables=df,
    blocking_rules=[
        block_on("substr(first_name, 1,1)", "surname"),
        block_on("surname"),
        block_on("email"),
        block_on("city", "first_name"),
        "l.first_name = r.first_name and levenshtein(l.surname, r.surname) < 2",
    ],
    db_api=db_api,
    link_type="dedupe_only",
).show()